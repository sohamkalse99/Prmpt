{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prmpt\n",
    "\n",
    "\n",
    "Team members:\n",
    "* Marcos Abadi\n",
    "* Leonardo Framba\n",
    "* Soham Kalse\n",
    "\n",
    "The goal of this project is to create a model that understands the relationship between English sentences and outputs a Unix command from a prompt. In order to achieve the desired outcome we had to preprocess the data in two different ways. \n",
    "* For the commands, we created an AST for each command and then followed to add generic tokens such as PATH, REGEX, NUMBER, etc. \n",
    "* For the invocations, we preprocessed the data by eliminating the stop words and lemmatizing the sentences, after that, we changed all of the directories to [PATH] for example: <br> cd /Desktop/School/ -> cd [PATH]\n",
    "\n",
    "\n",
    "The accuracy score measure is way of so don't be fooled by the 70% score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy as sp\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.language import Language\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "import keras_nlp\n",
    "import pathlib\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow_text.tools.wordpiece_vocab import (\n",
    "    bert_vocab_from_dataset as bert_vocab,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "EMBED_DIM = 50\n",
    "INTERMEDIATE_DIM = 512\n",
    "NUM_HEADS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(r\"CS663_Project_Data.json\", orient='index')\n",
    "data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invocation</th>\n",
       "      <th>cmd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Change directory to a different path</td>\n",
       "      <td>cd /path/to/directory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>List files in long format</td>\n",
       "      <td>ls -l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Print the physical path of the current directory</td>\n",
       "      <td>pwd -P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Create a new file</td>\n",
       "      <td>touch newfile.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Print a message to the console</td>\n",
       "      <td>echo \"Hello, world!\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28056</th>\n",
       "      <td>using exec in find command to dispaly the sear...</td>\n",
       "      <td>find . ... -exec cat {} \\; -exec echo \\;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28057</th>\n",
       "      <td>verbosely create intermediate directoriy tmp a...</td>\n",
       "      <td>mkdir -pv /tmp/boostinst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28058</th>\n",
       "      <td>view the manual page of find</td>\n",
       "      <td>man find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28059</th>\n",
       "      <td>wait 2 seconds and then print \"hello\"</td>\n",
       "      <td>echo \"hello `sleep 2 &amp;`\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28060</th>\n",
       "      <td>when using vi-insert keymap bind command \"\\C-v...</td>\n",
       "      <td>bind -m vi-insert '\"{\" \"\\C-v{}\\ei\"'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28060 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              invocation  \\\n",
       "1                   Change directory to a different path   \n",
       "2                              List files in long format   \n",
       "3       Print the physical path of the current directory   \n",
       "4                                      Create a new file   \n",
       "5                         Print a message to the console   \n",
       "...                                                  ...   \n",
       "28056  using exec in find command to dispaly the sear...   \n",
       "28057  verbosely create intermediate directoriy tmp a...   \n",
       "28058                       view the manual page of find   \n",
       "28059              wait 2 seconds and then print \"hello\"   \n",
       "28060  when using vi-insert keymap bind command \"\\C-v...   \n",
       "\n",
       "                                            cmd  \n",
       "1                         cd /path/to/directory  \n",
       "2                                         ls -l  \n",
       "3                                        pwd -P  \n",
       "4                             touch newfile.txt  \n",
       "5                          echo \"Hello, world!\"  \n",
       "...                                         ...  \n",
       "28056  find . ... -exec cat {} \\; -exec echo \\;  \n",
       "28057                  mkdir -pv /tmp/boostinst  \n",
       "28058                                  man find  \n",
       "28059                  echo \"hello `sleep 2 &`\"  \n",
       "28060       bind -m vi-insert '\"{\" \"\\C-v{}\\ei\"'  \n",
       "\n",
       "[28060 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate AST from command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting bashlex grammar using file: /Users/marcos/Desktop/School/Semester 4/CS663/CS663-Final-Project/bashlint/grammar/grammar100.txt\n",
      "Bashlint grammar set up (148 utilities)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bashlint import data_tools, lint\n",
    "\n",
    "df['cmd_ast'] = df['cmd'].apply(lint.normalize_ast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = []\n",
    "for i in range(1,len(df['cmd'])):\n",
    "    try:\n",
    "        df['cmd'][i] = data_tools.ast2template(df['cmd_ast'][i])\n",
    "    except:\n",
    "        drop.append(i)\n",
    "\n",
    "df.drop(drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words and preprocess invocation text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = sp.load(\"en_core_web_sm\")\n",
    "def process_text(row, column_name):\n",
    "   \n",
    "    text = row[column_name]\n",
    "    doc = nlp(text)\n",
    "    processed_tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"processed_text\"] = df.apply(process_text, axis=1, column_name='invocation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invocation</th>\n",
       "      <th>cmd</th>\n",
       "      <th>cmd_ast</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Change directory to a different path</td>\n",
       "      <td>cd Directory</td>\n",
       "      <td>&lt;bashlint.nast.Node object at 0x7f9a88d91670&gt;</td>\n",
       "      <td>change directory different path</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>List files in long format</td>\n",
       "      <td>ls -l</td>\n",
       "      <td>&lt;bashlint.nast.Node object at 0x7f9a88d93a60&gt;</td>\n",
       "      <td>list file long format</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Print the physical path of the current directory</td>\n",
       "      <td>pwd -P</td>\n",
       "      <td>&lt;bashlint.nast.Node object at 0x7f9a89aaf310&gt;</td>\n",
       "      <td>print physical path current directory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Create a new file</td>\n",
       "      <td>touch File</td>\n",
       "      <td>&lt;bashlint.nast.Node object at 0x7f9a89aafb20&gt;</td>\n",
       "      <td>create new file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Print a message to the console</td>\n",
       "      <td>echo Regex</td>\n",
       "      <td>&lt;bashlint.nast.Node object at 0x7f9a89aaf040&gt;</td>\n",
       "      <td>print message console</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28056</th>\n",
       "      <td>using exec in find command to dispaly the sear...</td>\n",
       "      <td>find Path Path -exec cat {} \\; -exec echo \\;</td>\n",
       "      <td>&lt;bashlint.nast.Node object at 0x7f9a898ff4c0&gt;</td>\n",
       "      <td>exec find command dispaly search file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28057</th>\n",
       "      <td>verbosely create intermediate directoriy tmp a...</td>\n",
       "      <td>mkdir -p -v Directory</td>\n",
       "      <td>&lt;bashlint.nast.Node object at 0x7f9a6e098ac0&gt;</td>\n",
       "      <td>verbosely create intermediate directoriy tmp r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28058</th>\n",
       "      <td>view the manual page of find</td>\n",
       "      <td>man Regex</td>\n",
       "      <td>&lt;bashlint.nast.Node object at 0x7f9a6dfdfa60&gt;</td>\n",
       "      <td>view manual page find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28059</th>\n",
       "      <td>wait 2 seconds and then print \"hello\"</td>\n",
       "      <td>echo $( sleep Timespan )</td>\n",
       "      <td>&lt;bashlint.nast.Node object at 0x7f9a8d5e1550&gt;</td>\n",
       "      <td>wait 2 second print hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28060</th>\n",
       "      <td>when using vi-insert keymap bind command \"\\C-v...</td>\n",
       "      <td>bind -m vi-insert '\"{\" \"\\C-v{}\\ei\"'</td>\n",
       "      <td>&lt;bashlint.nast.Node object at 0x7f9a70515460&gt;</td>\n",
       "      <td>vi insert keymap bind command \\c v{}\\ei key</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28031 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              invocation  \\\n",
       "1                   Change directory to a different path   \n",
       "2                              List files in long format   \n",
       "3       Print the physical path of the current directory   \n",
       "4                                      Create a new file   \n",
       "5                         Print a message to the console   \n",
       "...                                                  ...   \n",
       "28056  using exec in find command to dispaly the sear...   \n",
       "28057  verbosely create intermediate directoriy tmp a...   \n",
       "28058                       view the manual page of find   \n",
       "28059              wait 2 seconds and then print \"hello\"   \n",
       "28060  when using vi-insert keymap bind command \"\\C-v...   \n",
       "\n",
       "                                                cmd  \\\n",
       "1                                      cd Directory   \n",
       "2                                             ls -l   \n",
       "3                                            pwd -P   \n",
       "4                                        touch File   \n",
       "5                                        echo Regex   \n",
       "...                                             ...   \n",
       "28056  find Path Path -exec cat {} \\; -exec echo \\;   \n",
       "28057                         mkdir -p -v Directory   \n",
       "28058                                     man Regex   \n",
       "28059                      echo $( sleep Timespan )   \n",
       "28060           bind -m vi-insert '\"{\" \"\\C-v{}\\ei\"'   \n",
       "\n",
       "                                             cmd_ast  \\\n",
       "1      <bashlint.nast.Node object at 0x7f9a88d91670>   \n",
       "2      <bashlint.nast.Node object at 0x7f9a88d93a60>   \n",
       "3      <bashlint.nast.Node object at 0x7f9a89aaf310>   \n",
       "4      <bashlint.nast.Node object at 0x7f9a89aafb20>   \n",
       "5      <bashlint.nast.Node object at 0x7f9a89aaf040>   \n",
       "...                                              ...   \n",
       "28056  <bashlint.nast.Node object at 0x7f9a898ff4c0>   \n",
       "28057  <bashlint.nast.Node object at 0x7f9a6e098ac0>   \n",
       "28058  <bashlint.nast.Node object at 0x7f9a6dfdfa60>   \n",
       "28059  <bashlint.nast.Node object at 0x7f9a8d5e1550>   \n",
       "28060  <bashlint.nast.Node object at 0x7f9a70515460>   \n",
       "\n",
       "                                          processed_text  \n",
       "1                        change directory different path  \n",
       "2                                  list file long format  \n",
       "3                  print physical path current directory  \n",
       "4                                        create new file  \n",
       "5                                  print message console  \n",
       "...                                                  ...  \n",
       "28056              exec find command dispaly search file  \n",
       "28057  verbosely create intermediate directoriy tmp r...  \n",
       "28058                              view manual page find  \n",
       "28059                          wait 2 second print hello  \n",
       "28060        vi insert keymap bind command \\c v{}\\ei key  \n",
       "\n",
       "[28031 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_df = df.copy()\n",
    "tuple_df = df[['processed_text', 'cmd']]\n",
    "text_pairs = list(tuple_df.to_records(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28031 total pairs\n",
      "19623 training pairs\n",
      "4204 validation pairs\n",
      "4204 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
    "    word_piece_ds = tf.data.Dataset.from_tensor_slices(text_samples)\n",
    "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "        word_piece_ds.batch(1000).prefetch(2),\n",
    "        vocabulary_size=vocab_size,\n",
    "        reserved_tokens=reserved_tokens,\n",
    "    )\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% of invocations have length <= 16.0\n",
      "95% of commands have length <= 20.0\n",
      "7977 1740\n"
     ]
    }
   ],
   "source": [
    "inv_lengths = df[\"processed_text\"].apply(lambda x: len(x.split()))\n",
    "cmd_lengths = df[\"cmd\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "inv_95_quantile = inv_lengths.quantile(0.95)\n",
    "cmd_95_quantile = cmd_lengths.quantile(0.95)\n",
    "\n",
    "print(f\"95% of invocations have length <= {inv_95_quantile}\")\n",
    "print(f\"95% of commands have length <= {cmd_95_quantile}\")\n",
    "\n",
    "def get_unique_tokens(text_series):\n",
    "    all_tokens = []\n",
    "    for text in text_series:\n",
    "        tokens = text.split()\n",
    "        all_tokens.extend(tokens)\n",
    "    unique_tokens = set(all_tokens)\n",
    "    return unique_tokens\n",
    "\n",
    "inv_unique_tokens = get_unique_tokens(df[\"processed_text\"])\n",
    "cmd_unique_tokens = get_unique_tokens(df[\"cmd\"])\n",
    "\n",
    "print(len(inv_unique_tokens), len(cmd_unique_tokens))\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "INV_VOCAB_SIZE = 7977\n",
    "CMD_VOCAB_SIZE = 1740"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "inv_samples = [text_pair[0] for text_pair in train_pairs]\n",
    "inv_vocab = train_word_piece(inv_samples, INV_VOCAB_SIZE, reserved_tokens)\n",
    "\n",
    "cmd_samples = [text_pair[1] for text_pair in train_pairs]\n",
    "cmd_vocab = train_word_piece(cmd_samples, CMD_VOCAB_SIZE, reserved_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=inv_vocab, lowercase=False\n",
    ")\n",
    "cmd_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=cmd_vocab, lowercase=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(inv, cmd):\n",
    "    batch_size = tf.shape(cmd)[0]\n",
    "\n",
    "    inv = inv_tokenizer(inv)\n",
    "    cmd = cmd_tokenizer(cmd)\n",
    "\n",
    "    # Pad `invocation` to `MAX_SEQUENCE_LENGTH`.\n",
    "    inv_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "        pad_value=inv_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    inv = inv_start_end_packer(inv)\n",
    "\n",
    "    # Add special tokens (`\"[START]\"` and `\"[END]\"`) to `cmd` and pad it as well.\n",
    "    cmd_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
    "        start_value=cmd_tokenizer.token_to_id(\"[START]\"),\n",
    "        end_value=cmd_tokenizer.token_to_id(\"[END]\"),\n",
    "        pad_value=cmd_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    cmd = cmd_start_end_packer(cmd)\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            \"encoder_inputs\": inv,\n",
    "            \"decoder_inputs\": cmd[:, :-1],\n",
    "        },\n",
    "        cmd[:, 1:],\n",
    "    )\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    inv_texts, cmd_texts = zip(*pairs)\n",
    "    inv_texts = list(inv_texts)\n",
    "    cmd_texts = list(cmd_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inv_texts, cmd_texts))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (8, 100)\n",
      "inputs[\"decoder_inputs\"].shape: (8, 100)\n",
      "targets.shape: (8, 100)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=INV_VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")(encoder_inputs)\n",
    "\n",
    "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(inputs=x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=CMD_VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")(decoder_inputs)\n",
    "\n",
    "x = keras_nlp.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = keras.layers.Dense(CMD_VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "decoder = keras.Model(\n",
    "    [\n",
    "        decoder_inputs,\n",
    "        encoded_seq_inputs,\n",
    "    ],\n",
    "    decoder_outputs,\n",
    ")\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_outputs,\n",
    "    name=\"transformer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2453/2453 [==============================] - 177s 70ms/step - loss: 3.3939 - accuracy: 0.3675 - val_loss: 1.9400 - val_accuracy: 0.5559\n",
      "Epoch 2/10\n",
      "2453/2453 [==============================] - 169s 69ms/step - loss: 1.8622 - accuracy: 0.5617 - val_loss: 1.5343 - val_accuracy: 0.6084\n",
      "Epoch 3/10\n",
      "2453/2453 [==============================] - 166s 68ms/step - loss: 1.5842 - accuracy: 0.6050 - val_loss: 1.3572 - val_accuracy: 0.6431\n",
      "Epoch 4/10\n",
      "2453/2453 [==============================] - 165s 67ms/step - loss: 1.4388 - accuracy: 0.6321 - val_loss: 1.2502 - val_accuracy: 0.6645\n",
      "Epoch 5/10\n",
      "2453/2453 [==============================] - 166s 68ms/step - loss: 1.3413 - accuracy: 0.6531 - val_loss: 1.1709 - val_accuracy: 0.6816\n",
      "Epoch 6/10\n",
      "2453/2453 [==============================] - 167s 68ms/step - loss: 1.2662 - accuracy: 0.6702 - val_loss: 1.1060 - val_accuracy: 0.6970\n",
      "Epoch 7/10\n",
      "2453/2453 [==============================] - 164s 67ms/step - loss: 1.2052 - accuracy: 0.6840 - val_loss: 1.0587 - val_accuracy: 0.7103\n",
      "Epoch 8/10\n",
      "2453/2453 [==============================] - 178s 73ms/step - loss: 1.1568 - accuracy: 0.6957 - val_loss: 1.0170 - val_accuracy: 0.7225\n",
      "Epoch 9/10\n",
      "2453/2453 [==============================] - 160s 65ms/step - loss: 1.1137 - accuracy: 0.7061 - val_loss: 0.9853 - val_accuracy: 0.7296\n",
      "Epoch 10/10\n",
      "2453/2453 [==============================] - 154s 63ms/step - loss: 1.0750 - accuracy: 0.7151 - val_loss: 0.9578 - val_accuracy: 0.7371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9a8c442610>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "custom_lr = 1e-4\n",
    "optimizer = Adam(learning_rate=custom_lr)\n",
    "\n",
    "transformer.compile(\n",
    "    optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "transformer.fit(train_ds, epochs=10, validation_data=val_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code is from KerasNLP library, we could not figure out a way of correctly importing the version we needed to we simply copied the code in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 The KerasNLP Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Text generation utilities.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def _validate_prompt(prompt):\n",
    "    \"\"\"Helper function to validate input to text_generation utils.\"\"\"\n",
    "    if not isinstance(prompt, (tf.Tensor, tf.RaggedTensor)):\n",
    "        prompt = tf.convert_to_tensor(prompt)\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def _validate_token_probability_fn(token_probability_fn, prompt):\n",
    "    \"\"\"Helper function to validate `token_probability_fn` output.\"\"\"\n",
    "    test_pred = token_probability_fn(prompt)\n",
    "    if len(test_pred.shape) != 2:\n",
    "        raise ValueError(\n",
    "            \"Output of `token_probability_fn` is not a 2D tensor, \"\n",
    "            \"please provide a function with the output shape \"\n",
    "            \"[batch_size, vocab_size].\"\n",
    "        )\n",
    "\n",
    "    return tf.shape(test_pred)[-1], test_pred.dtype\n",
    "\n",
    "\n",
    "def _get_prompt_shape(prompt):\n",
    "    \"\"\"Helper function to get the batch size and prompt length.\"\"\"\n",
    "    if isinstance(prompt, tf.Tensor):\n",
    "        shape = tf.shape(prompt)\n",
    "        return (shape[0], shape[1])\n",
    "    elif isinstance(prompt, tf.RaggedTensor):\n",
    "        batch_size = prompt.nrows()\n",
    "        length = tf.math.reduce_min(tf.RaggedTensor.row_lengths(prompt))\n",
    "        return (batch_size, length)\n",
    "\n",
    "\n",
    "def _pad_prompt(prompt, max_length):\n",
    "    \"\"\"Pad prompt to `max_length` and compute a mask for controlled updates.\n",
    "\n",
    "    This utility will pad the (possibly ragged) prompt to `max_length`, and\n",
    "    compute a mask where the input was originally set in the prompt, to avoid\n",
    "    overwriting the original inputs when generating token(s) for the next\n",
    "    timestep.\n",
    "    \"\"\"\n",
    "    if isinstance(prompt, tf.Tensor):\n",
    "        shape = tf.shape(prompt)\n",
    "        extra_space = tf.math.maximum(0, max_length - shape[1])\n",
    "        pad_shape = [shape[0], extra_space]\n",
    "\n",
    "        mask = tf.ones(shape, tf.bool)\n",
    "        mask = tf.concat((mask, tf.zeros(pad_shape, tf.bool)), axis=1)\n",
    "        prompt = tf.concat((prompt, tf.zeros(pad_shape, prompt.dtype)), axis=1)\n",
    "    elif isinstance(prompt, tf.RaggedTensor):\n",
    "        # TODO: `to_tensor()` works with `jit_compile = True` in TF 2.8.x but\n",
    "        # fails in TF 2.9.x. Fix this. After this issue has been fixed, we can\n",
    "        # condense the two branches into one by starting off with a ragged tensor.\n",
    "        mask = tf.ones_like(prompt, dtype=tf.bool)\n",
    "        mask = mask.to_tensor(shape=(None, max_length))\n",
    "        prompt = prompt.to_tensor(shape=(None, max_length))\n",
    "    return prompt, mask\n",
    "\n",
    "\n",
    "def _mask_tokens_after_end_token(\n",
    "    prompt, max_length, end_token_id, pad_token_id\n",
    "):\n",
    "    \"\"\"Helper function to mask the tokens after the end token.\"\"\"\n",
    "    # Mask out tokens after `end_token_id` is encountered.\n",
    "    # Find index of first end_token_id.\n",
    "    end_indices = tf.math.argmax(prompt == end_token_id, -1)\n",
    "    # Use max_length if no `end_token_id` is found.\n",
    "    end_indices = tf.where(\n",
    "        end_indices == 0,\n",
    "        tf.cast(max_length, dtype=end_indices.dtype),\n",
    "        end_indices,\n",
    "    )\n",
    "    # Build a mask including end_token and replace tokens after end_token\n",
    "    # with `pad_token_id`.\n",
    "    valid_indices = tf.sequence_mask(end_indices + 1, maxlen=max_length)\n",
    "    return tf.where(valid_indices, prompt, pad_token_id)\n",
    "\n",
    "\n",
    "def greedy_search(\n",
    "    token_probability_fn,\n",
    "    prompt,\n",
    "    max_length,\n",
    "    end_token_id=None,\n",
    "    pad_token_id=0,\n",
    "):\n",
    "    \"\"\"Text generation utility based on greedy search.\n",
    "\n",
    "    Greedy search always appends the token having the largest probability to\n",
    "    existing sequence.\n",
    "\n",
    "    Args:\n",
    "        token_probability_fn: a callable, which takes in input_sequence\n",
    "            and output the probability distribution or the logits of the next\n",
    "            token.\n",
    "        prompt: a list or a Tensor, can be 1D or 2D, the initial tokens to\n",
    "            append generated tokens.\n",
    "        max_length: int. The max length of generated text.\n",
    "        end_token_id: int, defaults to None. The token marking the end of the\n",
    "            sequence, once encountered the generation is finished for the exact\n",
    "            sequence. If None, every sequence is generated up to `max_length`.\n",
    "            If set, all tokens after encountering `end_token_id` will be\n",
    "            replaced with `pad_token_id`.\n",
    "        pad_token_id: int, defaults to 0. The pad token after `end_token_id`\n",
    "            is received.\n",
    "\n",
    "    Returns:\n",
    "        A 1D int Tensor, or 2D int RaggedTensor representing the generated\n",
    "        sequences.\n",
    "\n",
    "    Examples:\n",
    "    ```python\n",
    "    BATCH_SIZE = 8\n",
    "    VOCAB_SIZE = 10\n",
    "    FEATURE_SIZE = 16\n",
    "    START_ID = 1\n",
    "    END_ID = 2\n",
    "\n",
    "    # Create a dummy model to predict the next token.\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=[None]),\n",
    "            keras.layers.Embedding(\n",
    "                input_dim=VOCAB_SIZE,\n",
    "                output_dim=FEATURE_SIZE,\n",
    "            ),\n",
    "            keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define a function that outputs the next token's probability given the\n",
    "    # input sequence.\n",
    "    def token_probability_fn(inputs):\n",
    "        return model(inputs)[:, -1, :]\n",
    "\n",
    "    prompt = tf.fill((BATCH_SIZE, 1), START_ID)\n",
    "\n",
    "    # Print the generated sequence (token ids).\n",
    "    keras_nlp.utils.greedy_search(\n",
    "        token_probability_fn,\n",
    "        prompt,\n",
    "        max_length=10,\n",
    "        end_token_id=END_ID,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "    prompt = _validate_prompt(prompt)\n",
    "\n",
    "    input_is_1d = prompt.shape.rank == 1\n",
    "    if input_is_1d:\n",
    "        prompt = prompt[tf.newaxis, :]\n",
    "\n",
    "    batch_size, length = _get_prompt_shape(prompt)\n",
    "    prompt, mask = _pad_prompt(prompt, max_length)\n",
    "\n",
    "    _validate_token_probability_fn(token_probability_fn, prompt)\n",
    "\n",
    "    def one_step(length, prompt):\n",
    "        pred = token_probability_fn(prompt[:, :length])\n",
    "        next_token = tf.cast(tf.argmax(pred, axis=-1), dtype=prompt.dtype)\n",
    "        next_token = tf.where(mask[:, length], prompt[:, length], next_token)\n",
    "\n",
    "        # Append the next token to current sequence.\n",
    "        prompt = tf.tensor_scatter_nd_update(\n",
    "            tensor=prompt,\n",
    "            indices=tf.stack(\n",
    "                (\n",
    "                    tf.cast(tf.range(batch_size), dtype=length.dtype),\n",
    "                    tf.repeat(length, batch_size),\n",
    "                ),\n",
    "                axis=1,\n",
    "            ),\n",
    "            updates=next_token,\n",
    "        )\n",
    "\n",
    "        length = tf.add(length, 1)\n",
    "        return (length, prompt)\n",
    "\n",
    "    # Run a while loop till text of length `max_length` has been generated.\n",
    "    length, prompt = tf.while_loop(\n",
    "        cond=lambda length, _: tf.less(length, max_length),\n",
    "        body=one_step,\n",
    "        loop_vars=(length, prompt),\n",
    "    )\n",
    "\n",
    "    if end_token_id is not None:\n",
    "        prompt = _mask_tokens_after_end_token(\n",
    "            prompt, max_length, end_token_id, pad_token_id\n",
    "        )\n",
    "\n",
    "    return tf.squeeze(prompt) if input_is_1d else prompt\n",
    "\n",
    "\n",
    "def beam_search(\n",
    "    token_probability_fn,\n",
    "    prompt,\n",
    "    max_length,\n",
    "    num_beams,\n",
    "    from_logits=False,\n",
    "    end_token_id=None,\n",
    "    pad_token_id=0,\n",
    "):\n",
    "    \"\"\"Text generation utility based on beam search algorithm.\n",
    "\n",
    "    At each time-step, beam search keeps the beams (sequences) of the top\n",
    "    `num_beams` highest accumulated probabilities, and uses each one of the\n",
    "    beams to predict candidate next tokens.\n",
    "\n",
    "    Args:\n",
    "        token_probability_fn: a callable, which takes in input_sequence\n",
    "            and outputs the probability distribution of the next token. If\n",
    "            `from_logits` set to True, it should output the logits of the next\n",
    "            token. The input shape would be `[batch_size * num_beams, length]`\n",
    "            and the output should be `[batch_size * num_beams, vocab_size]`.\n",
    "        prompt: a list or a Tensor, can be 1D or 2D, the initial tokens to\n",
    "            append generated tokens. The initial beam for beam search.\n",
    "        max_length: int. The max length of generated text.\n",
    "        num_beams: int. The number of beams that should be kept at each\n",
    "            time-step. `num_beams` should be strictly positive.\n",
    "        from_logits: bool. Indicates whether `token_probability_fn` outputs\n",
    "            logits or probabilities.\n",
    "        end_token_id: int, defaults to None. The token marking the end of the\n",
    "            sequence, once encountered the generation is finished for the exact\n",
    "            sequence. If None, every sequence is generated up to `max_length`.\n",
    "            If set, all tokens after encountering `end_token_id` will be\n",
    "            replaced with `pad_token_id`.\n",
    "        pad_token_id: int, defaults to 0. The pad token after `end_token_id`\n",
    "            is received.\n",
    "\n",
    "    Returns:\n",
    "        A 1D int Tensor, or 2D int Tensor representing the generated\n",
    "        sequences.\n",
    "\n",
    "    Examples:\n",
    "    ```python\n",
    "    BATCH_SIZE = 8\n",
    "    VOCAB_SIZE = 10\n",
    "    FEATURE_SIZE = 16\n",
    "    START_ID = 1\n",
    "    END_ID = 2\n",
    "\n",
    "    # Create a dummy model to predict the next token.\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=[None]),\n",
    "            keras.layers.Embedding(\n",
    "                input_dim=VOCAB_SIZE,\n",
    "                output_dim=FEATURE_SIZE,\n",
    "            ),\n",
    "            keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define a function that outputs the next token's probability given the\n",
    "    # input sequence.\n",
    "    def token_probability_fn(inputs):\n",
    "        return model(inputs)[:, -1, :]\n",
    "\n",
    "    prompt = tf.fill((BATCH_SIZE, 1), START_ID)\n",
    "\n",
    "    # Print the generated sequence (token ids).\n",
    "    keras_nlp.utils.beam_search(\n",
    "        token_probability_fn,\n",
    "        prompt,\n",
    "        max_length=10,\n",
    "        num_beams=5,\n",
    "        end_token_id=END_ID,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "    if num_beams <= 0:\n",
    "        raise ValueError(\n",
    "            f\"`num_beams` should be strictly positive. Received: `num_beams={num_beams}`.\"\n",
    "        )\n",
    "    if num_beams == 1:\n",
    "        return greedy_search(\n",
    "            token_probability_fn=token_probability_fn,\n",
    "            prompt=prompt,\n",
    "            max_length=max_length,\n",
    "            end_token_id=end_token_id,\n",
    "            pad_token_id=pad_token_id,\n",
    "        )\n",
    "\n",
    "    prompt = _validate_prompt(prompt)\n",
    "\n",
    "    input_is_1d = prompt.shape.rank == 1\n",
    "    if input_is_1d:\n",
    "        prompt = prompt[tf.newaxis, :]\n",
    "\n",
    "    batch_size, length = _get_prompt_shape(prompt)\n",
    "    prompt, mask = _pad_prompt(prompt, max_length)\n",
    "\n",
    "    vocab_size, pred_dtype = _validate_token_probability_fn(\n",
    "        token_probability_fn, prompt\n",
    "    )\n",
    "\n",
    "    if length >= max_length:\n",
    "        return tf.squeeze(prompt) if input_is_1d else prompt\n",
    "\n",
    "    # Initialize beam with shape `(batch_size, num_beams, length)`.\n",
    "    beams = tf.repeat(tf.expand_dims(prompt, axis=1), num_beams, axis=1)\n",
    "\n",
    "    # Initialize `beams_prob` with shape `(batch_size, num_beams)`.\n",
    "    beams_prob = tf.zeros([batch_size, 1], dtype=pred_dtype)\n",
    "    beams_prob = tf.concat(\n",
    "        [beams_prob, tf.fill((batch_size, num_beams - 1), pred_dtype.min)],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    def one_step(beams, beams_prob, length):\n",
    "        truncated_beams = beams[..., :length]\n",
    "\n",
    "        flattened_beams = tf.reshape(\n",
    "            truncated_beams, shape=[batch_size * num_beams, -1]\n",
    "        )\n",
    "        preds = token_probability_fn(flattened_beams)\n",
    "        if from_logits:\n",
    "            preds = keras.activations.softmax(preds, axis=-1)\n",
    "        # Reshape `preds` to shape `(batch_size, num_beams * vocab_size)`.\n",
    "        preds = tf.reshape(preds, shape=[batch_size, -1])\n",
    "\n",
    "        probs = tf.math.log(preds) + tf.repeat(\n",
    "            beams_prob, repeats=vocab_size, axis=1\n",
    "        )\n",
    "\n",
    "        candidate_prob, candidate_indexes = tf.math.top_k(\n",
    "            probs, k=num_beams, sorted=False\n",
    "        )\n",
    "        candidate_beam_indexes = candidate_indexes // vocab_size\n",
    "        next_token = candidate_indexes % vocab_size\n",
    "\n",
    "        beams = tf.gather(beams, candidate_beam_indexes, axis=1, batch_dims=1)\n",
    "\n",
    "        # Build a new column of updates to scatter into the beam tensor.\n",
    "        next_token = tf.where(\n",
    "            condition=mask[..., length, tf.newaxis],\n",
    "            x=beams[..., length],\n",
    "            y=next_token,\n",
    "        )\n",
    "        next_token = tf.reshape(next_token, shape=[-1])\n",
    "\n",
    "        # Generate `(batch_index, beam_index)` tuples for each beam.\n",
    "        beam_indices = tf.where(tf.ones((batch_size, num_beams), tf.bool))\n",
    "        beam_indices = tf.cast(beam_indices, dtype=length.dtype)\n",
    "        # Build a tensor of repeated `length` values.\n",
    "        length_indices = tf.fill((batch_size * num_beams, 1), length)\n",
    "        # Concatenate to a triplet of `(batch_index, beam_index, length)`.\n",
    "        indices = tf.concat([beam_indices, length_indices], axis=-1)\n",
    "\n",
    "        # Update `beams[:, :, length]` with `next_token`.\n",
    "        beams = tf.tensor_scatter_nd_update(\n",
    "            tensor=beams,\n",
    "            indices=indices,\n",
    "            updates=next_token,\n",
    "        )\n",
    "\n",
    "        beams_prob = candidate_prob\n",
    "        length = tf.add(length, 1)\n",
    "\n",
    "        return beams, beams_prob, length\n",
    "\n",
    "    # Run a while loop till text of length `max_length` has been generated.\n",
    "    beams, beams_prob, length = tf.while_loop(\n",
    "        cond=lambda beams, beams_prob, length: tf.less(length, max_length),\n",
    "        body=one_step,\n",
    "        loop_vars=(beams, beams_prob, length),\n",
    "    )\n",
    "\n",
    "    # Get the beam with the maximum probability.\n",
    "    max_indexes = tf.math.argmax(beams_prob, axis=-1)\n",
    "    max_beams = tf.gather(\n",
    "        beams, max_indexes[:, tf.newaxis], axis=1, batch_dims=1\n",
    "    )\n",
    "    prompt = tf.squeeze(max_beams)\n",
    "\n",
    "    if end_token_id is not None:\n",
    "        prompt = _mask_tokens_after_end_token(\n",
    "            prompt, max_length, end_token_id, pad_token_id\n",
    "        )\n",
    "    return tf.squeeze(prompt) if input_is_1d else prompt\n",
    "\n",
    "\n",
    "def random_search(\n",
    "    token_probability_fn,\n",
    "    prompt,\n",
    "    max_length,\n",
    "    seed=None,\n",
    "    from_logits=False,\n",
    "    end_token_id=None,\n",
    "    pad_token_id=0,\n",
    "):\n",
    "    \"\"\"Text generation utility based on randomly sampling the entire probability\n",
    "    distribution.\n",
    "\n",
    "    Random sampling samples the next token from the probability distribution\n",
    "    provided by `token_probability_fn` and appends it to the existing sequence.\n",
    "\n",
    "    Args:\n",
    "        token_probability_fn: a callable, which takes in input_sequence\n",
    "            and output the probability distribution of the next token. If\n",
    "            `from_logits` set to True, it should output the logits of the next\n",
    "            token.\n",
    "        prompt: a list or a Tensor, can be 1D or 2D, the initial tokens to\n",
    "            append generated tokens.\n",
    "        max_length: int. The max length of generated text.\n",
    "        seed: int, defaults to None. The random seed used for sampling.\n",
    "        from_logits: bool. Indicates whether `token_probability_fn` outputs\n",
    "            logits or probabilities.\n",
    "        end_token_id: int, defaults to None. The token marking the end of the\n",
    "            sequence, once encountered the generation is finished for the exact\n",
    "            sequence. If None, every sequence is generated up to `max_length`.\n",
    "            If set, all tokens after encountering `end_token_id` will be\n",
    "            replaced with `pad_token_id`.\n",
    "        pad_token_id: int, defaults to 0. The pad token after `end_token_id`\n",
    "            is received.\n",
    "\n",
    "    Returns:\n",
    "        A 1D int Tensor, or 2D int Tensor representing the generated\n",
    "        sequences.\n",
    "\n",
    "    Examples:\n",
    "    ```python\n",
    "    BATCH_SIZE = 8\n",
    "    VOCAB_SIZE = 10\n",
    "    FEATURE_SIZE = 16\n",
    "    START_ID = 1\n",
    "    END_ID = 2\n",
    "\n",
    "    # Create a dummy model to predict the next token.\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=[None]),\n",
    "            keras.layers.Embedding(\n",
    "                input_dim=VOCAB_SIZE,\n",
    "                output_dim=FEATURE_SIZE,\n",
    "            ),\n",
    "            keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define a function that outputs the next token's probability given the\n",
    "    # input sequence.\n",
    "    def token_probability_fn(inputs):\n",
    "        return model(inputs)[:, -1, :]\n",
    "\n",
    "    prompt = tf.fill((BATCH_SIZE, 1), START_ID)\n",
    "\n",
    "    # Print the generated sequence (token ids).\n",
    "    keras_nlp.utils.random_search(\n",
    "        token_probability_fn,\n",
    "        prompt,\n",
    "        max_length=10,\n",
    "        end_token_id=END_ID,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "    prompt = _validate_prompt(prompt)\n",
    "\n",
    "    input_is_1d = prompt.shape.rank == 1\n",
    "    if input_is_1d:\n",
    "        prompt = prompt[tf.newaxis, :]\n",
    "\n",
    "    batch_size, length = _get_prompt_shape(prompt)\n",
    "    prompt, mask = _pad_prompt(prompt, max_length)\n",
    "\n",
    "    _validate_token_probability_fn(token_probability_fn, prompt)\n",
    "\n",
    "    def one_step(length, prompt):\n",
    "        pred = token_probability_fn(prompt[:, :length])\n",
    "        if from_logits:\n",
    "            pred = keras.activations.softmax(pred, axis=-1)\n",
    "        next_token = tf.squeeze(\n",
    "            tf.cast(\n",
    "                tf.random.categorical(tf.math.log(pred), 1, seed=seed),\n",
    "                dtype=prompt.dtype,\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        next_token = tf.where(mask[:, length], prompt[:, length], next_token)\n",
    "\n",
    "        # Append the next token to current sequence.\n",
    "        prompt = tf.tensor_scatter_nd_update(\n",
    "            tensor=prompt,\n",
    "            indices=tf.stack(\n",
    "                (\n",
    "                    tf.cast(tf.range(batch_size), dtype=length.dtype),\n",
    "                    tf.repeat(length, batch_size),\n",
    "                ),\n",
    "                axis=1,\n",
    "            ),\n",
    "            updates=next_token,\n",
    "        )\n",
    "\n",
    "        length = tf.add(length, 1)\n",
    "        return (length, prompt)\n",
    "\n",
    "    # Run a while loop till text of length `max_length` has been generated.\n",
    "    length, prompt = tf.while_loop(\n",
    "        cond=lambda length, _: tf.less(length, max_length),\n",
    "        body=one_step,\n",
    "        loop_vars=(length, prompt),\n",
    "    )\n",
    "\n",
    "    if end_token_id is not None:\n",
    "        prompt = _mask_tokens_after_end_token(\n",
    "            prompt, max_length, end_token_id, pad_token_id\n",
    "        )\n",
    "\n",
    "    return tf.squeeze(prompt) if input_is_1d else prompt\n",
    "\n",
    "\n",
    "def top_k_search(\n",
    "    token_probability_fn,\n",
    "    prompt,\n",
    "    max_length,\n",
    "    k,\n",
    "    seed=None,\n",
    "    from_logits=False,\n",
    "    end_token_id=None,\n",
    "    pad_token_id=0,\n",
    "):\n",
    "    \"\"\"Text generation utility based on top-k sampling.\n",
    "\n",
    "    Top-k search samples the next token from the top-k tokens in the\n",
    "    probability distribution provided by `token_probability_fn` and appends it\n",
    "    to the existing sequence.\n",
    "\n",
    "    Args:\n",
    "        token_probability_fn: a callable, which takes in input_sequence\n",
    "            and output the probability distribution of the next token. If\n",
    "            `from_logits` set to True, it should output the logits of the next\n",
    "            token.\n",
    "        prompt: a list or a Tensor, can be 1D or 2D, the initial tokens to\n",
    "            append generated tokens.\n",
    "        max_length: int. The max length of generated text.\n",
    "        k: int. The number of top tokens to sample from. Should be non-negative\n",
    "            and less than the vocabulary size.\n",
    "        seed: int, defaults to None. The random seed used for sampling.\n",
    "        from_logits: bool. Indicates whether `token_probability_fn` outputs\n",
    "            logits or probabilities.\n",
    "        end_token_id: int, defaults to None. The token marking the end of the\n",
    "            sequence, once encountered the generation is finished for the exact\n",
    "            sequence. If None, every sequence is generated up to `max_length`.\n",
    "            If set, all tokens after encountering `end_token_id` will be\n",
    "            replaced with `pad_token_id`.\n",
    "        pad_token_id: int, defaults to 0. The pad token after `end_token_id`\n",
    "            is received.\n",
    "\n",
    "    Returns:\n",
    "        A 1D int Tensor, or 2D int Tensor representing the generated\n",
    "        sequences.\n",
    "\n",
    "    Examples:\n",
    "    ```python\n",
    "    BATCH_SIZE = 8\n",
    "    VOCAB_SIZE = 10\n",
    "    FEATURE_SIZE = 16\n",
    "    START_ID = 1\n",
    "    END_ID = 2\n",
    "\n",
    "    # Create a dummy model to predict the next token.\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=[None]),\n",
    "            keras.layers.Embedding(\n",
    "                input_dim=VOCAB_SIZE,\n",
    "                output_dim=FEATURE_SIZE,\n",
    "            ),\n",
    "            keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define a function that outputs the next token's probability given the\n",
    "    # input sequence.\n",
    "    def token_probability_fn(inputs):\n",
    "        return model(inputs)[:, -1, :]\n",
    "\n",
    "    prompt = tf.fill((BATCH_SIZE, 1), START_ID)\n",
    "\n",
    "    # Print the generated sequence (token ids).\n",
    "    keras_nlp.utils.top_k_search(\n",
    "        token_probability_fn,\n",
    "        prompt,\n",
    "        max_length=10,\n",
    "        k=4,\n",
    "        end_token_id=END_ID,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "    if k <= 0:\n",
    "        raise ValueError(f\"`k` should be strictly positive. Received: `k={k}`.\")\n",
    "\n",
    "    prompt = _validate_prompt(prompt)\n",
    "\n",
    "    input_is_1d = prompt.shape.rank == 1\n",
    "    if input_is_1d:\n",
    "        prompt = prompt[tf.newaxis, :]\n",
    "\n",
    "    batch_size, length = _get_prompt_shape(prompt)\n",
    "    prompt, mask = _pad_prompt(prompt, max_length)\n",
    "\n",
    "    _validate_token_probability_fn(token_probability_fn, prompt)\n",
    "\n",
    "    # If k is greater than the vocabulary size, use the entire vocabulary.\n",
    "    pred = token_probability_fn(prompt)\n",
    "    if k > pred.shape[1]:\n",
    "        logging.warning(\n",
    "            f\"`k` larger than vocabulary size={pred.shape[1]}.\"\n",
    "            f\"Setting `k` to vocabulary size. Received: `k={k}`.\"\n",
    "        )\n",
    "        k = pred.shape[1]\n",
    "\n",
    "    def one_step(length, prompt):\n",
    "        pred = token_probability_fn(prompt[:, :length])\n",
    "        if from_logits:\n",
    "            pred = keras.activations.softmax(pred, axis=-1)\n",
    "\n",
    "        # Filter out top-k tokens.\n",
    "        top_k_pred, top_k_indices = tf.math.top_k(pred, k=k, sorted=False)\n",
    "        # Sample the next token from the probability distribution.\n",
    "        next_token = tf.random.categorical(\n",
    "            tf.math.log(top_k_pred), 1, seed=seed\n",
    "        )\n",
    "\n",
    "        # Rearrange to get the next token idx from the original order.\n",
    "        next_token = tf.gather_nd(top_k_indices, next_token, batch_dims=1)\n",
    "        next_token = tf.cast(next_token, dtype=prompt.dtype)\n",
    "        next_token = tf.where(mask[:, length], prompt[:, length], next_token)\n",
    "\n",
    "        # Append the next token to current sequence.\n",
    "        prompt = tf.tensor_scatter_nd_update(\n",
    "            tensor=prompt,\n",
    "            indices=tf.stack(\n",
    "                (\n",
    "                    tf.cast(tf.range(batch_size), dtype=length.dtype),\n",
    "                    tf.repeat(length, batch_size),\n",
    "                ),\n",
    "                axis=1,\n",
    "            ),\n",
    "            updates=next_token,\n",
    "        )\n",
    "\n",
    "        length = tf.add(length, 1)\n",
    "        return (length, prompt)\n",
    "\n",
    "    # Run a while loop till text of length `max_length` has been generated.\n",
    "    length, prompt = tf.while_loop(\n",
    "        cond=lambda length, _: tf.less(length, max_length),\n",
    "        body=one_step,\n",
    "        loop_vars=(length, prompt),\n",
    "    )\n",
    "\n",
    "    if end_token_id is not None:\n",
    "        prompt = _mask_tokens_after_end_token(\n",
    "            prompt, max_length, end_token_id, pad_token_id\n",
    "        )\n",
    "\n",
    "    return tf.squeeze(prompt) if input_is_1d else prompt\n",
    "\n",
    "\n",
    "def top_p_search(\n",
    "    token_probability_fn,\n",
    "    prompt,\n",
    "    max_length,\n",
    "    p,\n",
    "    seed=None,\n",
    "    from_logits=False,\n",
    "    end_token_id=None,\n",
    "    pad_token_id=0,\n",
    "):\n",
    "    \"\"\"Text generation utility based on top-p (nucleus) sampling.\n",
    "\n",
    "    Top-p search selects tokens from the smallest subset of output probabilities\n",
    "    that sum to greater than `p`. Put another way, top-p will first order\n",
    "    token predictions by likelihood, and ignore all tokens after the cumulative\n",
    "    probability of selected tokens exceeds `p`. The probability of each\n",
    "    token is provided by `token_probability_fn`.\n",
    "\n",
    "    Args:\n",
    "        token_probability_fn: a callable, which takes in input_sequence\n",
    "            and output the probability distribution of the next token. If\n",
    "            `from_logits` set to True, it should output the logits of the next\n",
    "            token.\n",
    "        prompt: a list or a Tensor, can be 1D or 2D, the initial tokens to\n",
    "            append generated tokens.\n",
    "        max_length: int. The max length of generated text.\n",
    "        p: float. The probability that the top tokens sums up to. Should\n",
    "            follow the constraint of 0 < p < 1.\n",
    "        seed: int, defaults to None. The random seed used for sampling.\n",
    "        from_logits: bool. Indicates whether `token_probability_fn` outputs\n",
    "            logits or probabilities.\n",
    "        end_token_id: int, defaults to None. The token marking the end of the\n",
    "            sequence, once encountered the generation is finished for the exact\n",
    "            sequence. If None, every sequence is generated up to `max_length`.\n",
    "            If set, all tokens after encountering `end_token_id` will be\n",
    "            replaced with `pad_token_id`.\n",
    "        pad_token_id: int, defaults to 0. The pad token after `end_token_id`\n",
    "            is received.\n",
    "\n",
    "    Returns:\n",
    "        A 1D int Tensor, or 2D int Tensor representing the generated\n",
    "        sequences.\n",
    "\n",
    "    Examples:\n",
    "    ```python\n",
    "    BATCH_SIZE = 8\n",
    "    VOCAB_SIZE = 10\n",
    "    FEATURE_SIZE = 16\n",
    "    START_ID = 1\n",
    "    END_ID = 2\n",
    "\n",
    "    # Create a dummy model to predict the next token.\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=[None]),\n",
    "            keras.layers.Embedding(\n",
    "                input_dim=VOCAB_SIZE,\n",
    "                output_dim=FEATURE_SIZE,\n",
    "            ),\n",
    "            keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define a function that outputs the next token's probability given the\n",
    "    # input sequence.\n",
    "    def token_probability_fn(inputs):\n",
    "        return model(inputs)[:, -1, :]\n",
    "\n",
    "    prompt = tf.fill((BATCH_SIZE, 1), START_ID)\n",
    "\n",
    "    # Print the generated sequence (token ids).\n",
    "    keras_nlp.utils.top_p_search(\n",
    "        token_probability_fn,\n",
    "        prompt,\n",
    "        max_length=10,\n",
    "        p=0.8,\n",
    "        end_token_id=END_ID,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "    if p <= 0 or p >= 1:\n",
    "        raise ValueError(\n",
    "            f\"`p` should be in the range (0, 1). Received: `p={p}`.\"\n",
    "        )\n",
    "\n",
    "    prompt = _validate_prompt(prompt)\n",
    "\n",
    "    input_is_1d = prompt.shape.rank == 1\n",
    "    if input_is_1d:\n",
    "        prompt = prompt[tf.newaxis, :]\n",
    "\n",
    "    batch_size, length = _get_prompt_shape(prompt)\n",
    "    prompt, mask = _pad_prompt(prompt, max_length)\n",
    "\n",
    "    _validate_token_probability_fn(token_probability_fn, prompt)\n",
    "\n",
    "    def one_step(length, prompt):\n",
    "        pred = token_probability_fn(prompt[:, :length])\n",
    "        if from_logits:\n",
    "            pred = keras.activations.softmax(pred, axis=-1)\n",
    "        # Sort preds in descending order.\n",
    "        sorted_preds, sorted_indices = tf.math.top_k(\n",
    "            pred, k=pred.shape[1], sorted=True\n",
    "        )\n",
    "        # Calculate cumulative probability distribution.\n",
    "        cumulative_probs = tf.math.cumsum(sorted_preds, axis=-1)\n",
    "        # Create a mask for the tokens to keep.\n",
    "        keep_mask = cumulative_probs <= p\n",
    "        # Shift to include the last token that exceed p.\n",
    "        shifted_keep_mask = tf.concat(\n",
    "            [tf.ones_like(keep_mask[:, :1]), keep_mask[:, :-1]], axis=-1\n",
    "        )\n",
    "        # Filter out unmasked tokens and sample from filtered distribution.\n",
    "        probs = tf.where(\n",
    "            shifted_keep_mask,\n",
    "            sorted_preds,\n",
    "            tf.zeros(tf.shape(pred), dtype=sorted_preds.dtype),\n",
    "        )\n",
    "        sorted_next_token = tf.random.categorical(\n",
    "            tf.math.log(probs), 1, seed=seed\n",
    "        )\n",
    "        next_token = tf.gather_nd(\n",
    "            sorted_indices, sorted_next_token, batch_dims=1\n",
    "        )\n",
    "        next_token = tf.cast(next_token, dtype=prompt.dtype)\n",
    "        next_token = tf.where(mask[:, length], prompt[:, length], next_token)\n",
    "\n",
    "        # Append the next token to current sequence.\n",
    "        prompt = tf.tensor_scatter_nd_update(\n",
    "            tensor=prompt,\n",
    "            indices=tf.stack(\n",
    "                (\n",
    "                    tf.cast(tf.range(batch_size), dtype=length.dtype),\n",
    "                    tf.repeat(length, batch_size),\n",
    "                ),\n",
    "                axis=1,\n",
    "            ),\n",
    "            updates=next_token,\n",
    "        )\n",
    "\n",
    "        length = tf.add(length, 1)\n",
    "        return (length, prompt)\n",
    "\n",
    "    # Run a while loop till text of length `max_length` has been generated.\n",
    "    length, prompt = tf.while_loop(\n",
    "        cond=lambda length, _: tf.less(length, max_length),\n",
    "        body=one_step,\n",
    "        loop_vars=(length, prompt),\n",
    "    )\n",
    "\n",
    "    if end_token_id is not None:\n",
    "        prompt = _mask_tokens_after_end_token(\n",
    "            prompt, max_length, end_token_id, pad_token_id\n",
    "        )\n",
    "\n",
    "    return tf.squeeze(prompt) if input_is_1d else prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequences(input_sentences):\n",
    "    \n",
    "    batch_size = tf.shape(input_sentences)[0]\n",
    "\n",
    "    # Tokenize the encoder input.\n",
    "    encoder_input_tokens = inv_tokenizer(input_sentences).to_tensor(\n",
    "        shape=(None, MAX_SEQUENCE_LENGTH)\n",
    "    )\n",
    "\n",
    "    # Define a function that outputs the next token's probability given the\n",
    "    # input sequence.\n",
    "    def token_probability_fn(decoder_input_tokens):\n",
    "        return transformer([encoder_input_tokens, decoder_input_tokens])[:, -1, :]\n",
    "\n",
    "    # Set the prompt to the \"[START]\" token.\n",
    "    prompt = tf.fill((batch_size, 1), inv_tokenizer.token_to_id(\"[START]\"))\n",
    "\n",
    "    generated_tokens = top_p_search(\n",
    "        token_probability_fn,\n",
    "        prompt,\n",
    "        p=0.1,\n",
    "        max_length=40,\n",
    "        end_token_id=cmd_tokenizer.token_to_id(\"[END]\"),\n",
    "    )\n",
    "    generated_sentences = cmd_tokenizer.detokenize(generated_tokens)\n",
    "    return generated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Example 0 **\n",
      "delete file name filename current directory tree path end path\n",
      "find Path - name Regex - exec rm { } \\ ;\n",
      "\n",
      "** Example 1 **\n",
      "display long listing file entire file system\n",
      "find Path - type f - exec ls - l - l { } \\ ;\n",
      "\n",
      "** Example 2 **\n",
      "find string alphanumeric character second field output command path\n",
      "grep - i - f File | cut - d Regex - d Regex | head - c Quantity\n",
      "\n",
      "** Example 3 **\n",
      "print random number 100 999 echo number print content randomly choose file 32 character long\n",
      "shuf - i File - n File | xargs - I { } echo Regex { }\n",
      "\n",
      "** Example 4 **\n",
      "save short host append .mysqldb variable dbprefix\n",
      "mkdir - v Regex\n",
      "\n",
      "** Example 5 **\n",
      "find file current directory subdirectory execute command grep -l text\n",
      "find Path - type f - exec grep - l Regex { } \\ ;\n",
      "\n",
      "** Example 6 **\n",
      "search current directory tree file name end rb js\n",
      "find Path - name Regex - name Regex - exec grep - l Regex { } \\ ;\n",
      "\n",
      "** Example 7 **\n",
      "remove file current folder 5 new one ignore folder file list\n",
      "find Path - type f - name Regex - exec rm - l { } \\ ;\n",
      "\n",
      "** Example 8 **\n",
      "find file extension .txt delete\n",
      "find Path - name Regex - exec rm - f { } \\ ;\n",
      "\n",
      "** Example 9 **\n",
      "search file extension .txt current directory subdirectory print line contain 5 digit follow 5 letter\n",
      "find Path - name Regex | xargs - I { } grep - l Regex { }\n",
      "\n",
      "** Example 10 **\n",
      "recall second argument previous command press alt shift y\n",
      "echo Regex | grep - n File\n",
      "\n",
      "** Example 11 **\n",
      "find file current directory subdirectory path directory\n",
      "find Path - type f - exec mv { } File \\ ;\n",
      "\n",
      "** Example 12 **\n",
      "search path file modify 10 minute\n",
      "find Path - mmin - Quantity - type f - exec mv { } \\ ;\n",
      "\n",
      "** Example 13 **\n",
      "print random string 128 character compose alphanumeric character\n",
      "echo $ ( cat File | tr - d - c Regex | fold - w Quantity | head - n Quantity )\n",
      "\n",
      "** Example 14 **\n",
      "find randomly choose .jpg file current directory search case insensitively\n",
      "find Path - name Regex | xargs - I { } grep - i - r - r - f { }\n",
      "\n",
      "** Example 15 **\n",
      "switch user username\n",
      "echo Regex\n",
      "\n",
      "** Example 16 **\n",
      "cut fourth fifth field separate comma path file use awk increment second field print second field second field equal 15\n",
      "cut - d Regex - f Number File | sort - R | head - Quantity\n",
      "\n",
      "** Example 17 **\n",
      "display 100 line file-with-line-too-long.txt wait user input page\n",
      "find Path - type f - exec grep - i - l { } \\ ;\n",
      "\n",
      "** Example 18 **\n",
      "print output command generate 15 random alphanumeric character\n",
      "echo $ ( cat File | tr - d - c Regex | fold - w Quantity | head - n Quantity )\n",
      "\n",
      "** Example 19 **\n",
      "randomly select number range 1 20\n",
      "shuf - i File - n File | xargs - I { } echo Regex\n",
      "\n",
      "** Example 20 **\n",
      "find file extension .pdf current directory subdirectory delete directory match criterion\n",
      "find Path - name Regex - exec rm - f { } \\ ;\n",
      "\n",
      "** Example 21 **\n",
      "copy randomly choose file directory current directory home directory\n",
      "cp $ ( find Path - name Regex - exec cp - l File File File \\ ; )\n",
      "\n",
      "** Example 22 **\n",
      "find file extension .jpg current directory subdirectory file pictures directory home directory\n",
      "find Path - name Regex - exec mv { } File \\ ;\n",
      "\n",
      "** Example 23 **\n",
      "find file extension .jpg current directory subdirectory folder newfolder home directory\n",
      "find Path - name Regex - exec mv { } File \\ ;\n",
      "\n",
      "** Example 24 **\n",
      "find file end .zip path directory own user root remove write permission group user file\n",
      "find Path - user Regex - exec chmod Permission { } \\ ;\n",
      "\n",
      "** Example 25 **\n",
      "search current directory tree regular file change 10th september\n",
      "find Path - type f - exec sed - i Program { } \\ ;\n",
      "\n",
      "** Example 26 **\n",
      "find file extension .jpg path directory compress tarball name output.tar.gz 8 process parallel\n",
      "find Path - name Regex | xargs - I { } tar - c - z - f File { }\n",
      "\n",
      "** Example 27 **\n",
      "login user postgre\n",
      "read - n Quantity\n",
      "\n",
      "** Example 28 **\n",
      "print 100 character random datum path file echo output data\n",
      "shuf - i File - n File | xargs - I { } echo Regex { }\n",
      "\n",
      "** Example 29 **\n",
      "find file end f\n",
      "find Path - name Regex\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_inv_texts = [pair[0] for pair in test_pairs]\n",
    "for i in range(30):\n",
    "    input_sentence = random.choice(test_inv_texts)\n",
    "    translated = decode_sequences(tf.constant([input_sentence]))\n",
    "    translated = translated.numpy()[0].decode(\"utf-8\")\n",
    "    translated = (\n",
    "        translated.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    print(f\"** Example {i} **\")\n",
    "    print(input_sentence)\n",
    "    print(translated)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
