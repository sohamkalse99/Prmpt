{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!pip install -q rouge-score\n",
    "!!pip install -q git+https://github.com/keras-team/keras-nlp.git --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "import pathlib\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow_text.tools.wordpiece_vocab import (\n",
    "    bert_vocab_from_dataset as bert_vocab,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1  # This should be at least 10 for convergence\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "ENG_VOCAB_SIZE = 15000\n",
    "SPA_VOCAB_SIZE = 15000\n",
    "\n",
    "EMBED_DIM = 256\n",
    "INTERMEDIATE_DIM = 2048\n",
    "NUM_HEADS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "2638744/2638744 [==============================] - 2s 1us/step\n"
     ]
    }
   ],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname=\"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    eng = eng.lower()\n",
    "    spa = spa.lower()\n",
    "    text_pairs.append((eng, spa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118964 total pairs\n",
      "83276 training pairs\n",
      "17844 validation pairs\n",
      "17844 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
    "    word_piece_ds = tf.data.Dataset.from_tensor_slices(text_samples)\n",
    "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "        word_piece_ds.batch(1000).prefetch(2),\n",
    "        vocabulary_size=vocab_size,\n",
    "        reserved_tokens=reserved_tokens,\n",
    "    )\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "eng_samples = [text_pair[0] for text_pair in train_pairs]\n",
    "eng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)\n",
    "\n",
    "spa_samples = [text_pair[1] for text_pair in train_pairs]\n",
    "spa_vocab = train_word_piece(spa_samples, SPA_VOCAB_SIZE, reserved_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Tokens:  ['him', 'there', 'they', 'go', 'her', 'has', 're', 'will', 'll', 'time']\n",
      "Spanish Tokens:  ['te', 'para', 'mary', 'las', 'm√°s', 'al', 'yo', 'estoy', 'muy', 'tu']\n"
     ]
    }
   ],
   "source": [
    "print(\"English Tokens: \", eng_vocab[100:110])\n",
    "print(\"Spanish Tokens: \", spa_vocab[100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=eng_vocab, lowercase=False\n",
    ")\n",
    "spa_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=spa_vocab, lowercase=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence:  my hands are tied.\n",
      "Tokens:  tf.Tensor([  79  612   86 2140   12], shape=(5,), dtype=int32)\n",
      "Recovered text after detokenizing:  tf.Tensor(b'my hands are tied .', shape=(), dtype=string)\n",
      "\n",
      "Spanish sentence:  tengo las manos atadas.\n",
      "Tokens:  tf.Tensor([ 117  103  640   29 2063   91   15], shape=(7,), dtype=int32)\n",
      "Recovered text after detokenizing:  tf.Tensor(b'tengo las manos atadas .', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "eng_input_ex = text_pairs[0][0]\n",
    "eng_tokens_ex = eng_tokenizer.tokenize(eng_input_ex)\n",
    "print(\"English sentence: \", eng_input_ex)\n",
    "print(\"Tokens: \", eng_tokens_ex)\n",
    "print(\n",
    "    \"Recovered text after detokenizing: \",\n",
    "    eng_tokenizer.detokenize(eng_tokens_ex),\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "spa_input_ex = text_pairs[0][1]\n",
    "spa_tokens_ex = spa_tokenizer.tokenize(spa_input_ex)\n",
    "print(\"Spanish sentence: \", spa_input_ex)\n",
    "print(\"Tokens: \", spa_tokens_ex)\n",
    "print(\n",
    "    \"Recovered text after detokenizing: \",\n",
    "    spa_tokenizer.detokenize(spa_tokens_ex),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(eng, spa):\n",
    "    batch_size = tf.shape(spa)[0]\n",
    "\n",
    "    eng = eng_tokenizer(eng)\n",
    "    spa = spa_tokenizer(spa)\n",
    "\n",
    "    # Pad `eng` to `MAX_SEQUENCE_LENGTH`.\n",
    "    eng_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "        pad_value=eng_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    eng = eng_start_end_packer(eng)\n",
    "\n",
    "    # Add special tokens (`\"[START]\"` and `\"[END]\"`) to `spa` and pad it as well.\n",
    "    spa_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
    "        start_value=spa_tokenizer.token_to_id(\"[START]\"),\n",
    "        end_value=spa_tokenizer.token_to_id(\"[END]\"),\n",
    "        pad_value=spa_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    spa = spa_start_end_packer(spa)\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            \"encoder_inputs\": eng,\n",
    "            \"decoder_inputs\": spa[:, :-1],\n",
    "        },\n",
    "        spa[:, 1:],\n",
    "    )\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (64, 40)\n",
      "inputs[\"decoder_inputs\"].shape: (64, 40)\n",
      "targets.shape: (64, 40)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=ENG_VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")(encoder_inputs)\n",
    "\n",
    "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(inputs=x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=SPA_VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    mask_zero=True,\n",
    ")(decoder_inputs)\n",
    "\n",
    "x = keras_nlp.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = keras.layers.Dense(SPA_VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "decoder = keras.Model(\n",
    "    [\n",
    "        decoder_inputs,\n",
    "        encoded_seq_inputs,\n",
    "    ],\n",
    "    decoder_outputs,\n",
    ")\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_outputs,\n",
    "    name=\"transformer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " token_and_position_embedding (  (None, None, 256)   3850240     ['encoder_inputs[0][0]']         \n",
      " TokenAndPositionEmbedding)                                                                       \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 256)   1315072     ['token_and_position_embedding[0]\n",
      " erEncoder)                                                      [0]']                            \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, None, 15000)  9283992     ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,449,304\n",
      "Trainable params: 14,449,304\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "1302/1302 [==============================] - 1149s 879ms/step - loss: 3.8640 - accuracy: 0.4205 - val_loss: 2.9412 - val_accuracy: 0.5232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9ca9dc7a60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 The KerasNLP Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Text generation utilities.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def _validate_prompt(prompt):\n",
    "    \"\"\"Helper function to validate input to text_generation utils.\"\"\"\n",
    "    if not isinstance(prompt, (tf.Tensor, tf.RaggedTensor)):\n",
    "        prompt = tf.convert_to_tensor(prompt)\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def _validate_token_probability_fn(token_probability_fn, prompt):\n",
    "    \"\"\"Helper function to validate `token_probability_fn` output.\"\"\"\n",
    "    test_pred = token_probability_fn(prompt)\n",
    "    if len(test_pred.shape) != 2:\n",
    "        raise ValueError(\n",
    "            \"Output of `token_probability_fn` is not a 2D tensor, \"\n",
    "            \"please provide a function with the output shape \"\n",
    "            \"[batch_size, vocab_size].\"\n",
    "        )\n",
    "\n",
    "    return tf.shape(test_pred)[-1], test_pred.dtype\n",
    "\n",
    "\n",
    "def _get_prompt_shape(prompt):\n",
    "    \"\"\"Helper function to get the batch size and prompt length.\"\"\"\n",
    "    if isinstance(prompt, tf.Tensor):\n",
    "        shape = tf.shape(prompt)\n",
    "        return (shape[0], shape[1])\n",
    "    elif isinstance(prompt, tf.RaggedTensor):\n",
    "        batch_size = prompt.nrows()\n",
    "        length = tf.math.reduce_min(tf.RaggedTensor.row_lengths(prompt))\n",
    "        return (batch_size, length)\n",
    "\n",
    "\n",
    "def _pad_prompt(prompt, max_length):\n",
    "    \"\"\"Pad prompt to `max_length` and compute a mask for controlled updates.\n",
    "\n",
    "    This utility will pad the (possibly ragged) prompt to `max_length`, and\n",
    "    compute a mask where the input was originally set in the prompt, to avoid\n",
    "    overwriting the original inputs when generating token(s) for the next\n",
    "    timestep.\n",
    "    \"\"\"\n",
    "    if isinstance(prompt, tf.Tensor):\n",
    "        shape = tf.shape(prompt)\n",
    "        extra_space = tf.math.maximum(0, max_length - shape[1])\n",
    "        pad_shape = [shape[0], extra_space]\n",
    "\n",
    "        mask = tf.ones(shape, tf.bool)\n",
    "        mask = tf.concat((mask, tf.zeros(pad_shape, tf.bool)), axis=1)\n",
    "        prompt = tf.concat((prompt, tf.zeros(pad_shape, prompt.dtype)), axis=1)\n",
    "    elif isinstance(prompt, tf.RaggedTensor):\n",
    "        # TODO: `to_tensor()` works with `jit_compile = True` in TF 2.8.x but\n",
    "        # fails in TF 2.9.x. Fix this. After this issue has been fixed, we can\n",
    "        # condense the two branches into one by starting off with a ragged tensor.\n",
    "        mask = tf.ones_like(prompt, dtype=tf.bool)\n",
    "        mask = mask.to_tensor(shape=(None, max_length))\n",
    "        prompt = prompt.to_tensor(shape=(None, max_length))\n",
    "    return prompt, mask\n",
    "\n",
    "\n",
    "def _mask_tokens_after_end_token(\n",
    "    prompt, max_length, end_token_id, pad_token_id\n",
    "):\n",
    "    \"\"\"Helper function to mask the tokens after the end token.\"\"\"\n",
    "    # Mask out tokens after `end_token_id` is encountered.\n",
    "    # Find index of first end_token_id.\n",
    "    end_indices = tf.math.argmax(prompt == end_token_id, -1)\n",
    "    # Use max_length if no `end_token_id` is found.\n",
    "    end_indices = tf.where(\n",
    "        end_indices == 0,\n",
    "        tf.cast(max_length, dtype=end_indices.dtype),\n",
    "        end_indices,\n",
    "    )\n",
    "    # Build a mask including end_token and replace tokens after end_token\n",
    "    # with `pad_token_id`.\n",
    "    valid_indices = tf.sequence_mask(end_indices + 1, maxlen=max_length)\n",
    "    return tf.where(valid_indices, prompt, pad_token_id)\n",
    "\n",
    "\n",
    "def greedy_search(\n",
    "    token_probability_fn,\n",
    "    prompt,\n",
    "    max_length,\n",
    "    end_token_id=None,\n",
    "    pad_token_id=0,\n",
    "):\n",
    "    \"\"\"Text generation utility based on greedy search.\n",
    "\n",
    "    Greedy search always appends the token having the largest probability to\n",
    "    existing sequence.\n",
    "\n",
    "    Args:\n",
    "        token_probability_fn: a callable, which takes in input_sequence\n",
    "            and output the probability distribution or the logits of the next\n",
    "            token.\n",
    "        prompt: a list or a Tensor, can be 1D or 2D, the initial tokens to\n",
    "            append generated tokens.\n",
    "        max_length: int. The max length of generated text.\n",
    "        end_token_id: int, defaults to None. The token marking the end of the\n",
    "            sequence, once encountered the generation is finished for the exact\n",
    "            sequence. If None, every sequence is generated up to `max_length`.\n",
    "            If set, all tokens after encountering `end_token_id` will be\n",
    "            replaced with `pad_token_id`.\n",
    "        pad_token_id: int, defaults to 0. The pad token after `end_token_id`\n",
    "            is received.\n",
    "\n",
    "    Returns:\n",
    "        A 1D int Tensor, or 2D int RaggedTensor representing the generated\n",
    "        sequences.\n",
    "\n",
    "    Examples:\n",
    "    ```python\n",
    "    BATCH_SIZE = 8\n",
    "    VOCAB_SIZE = 10\n",
    "    FEATURE_SIZE = 16\n",
    "    START_ID = 1\n",
    "    END_ID = 2\n",
    "\n",
    "    # Create a dummy model to predict the next token.\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=[None]),\n",
    "            keras.layers.Embedding(\n",
    "                input_dim=VOCAB_SIZE,\n",
    "                output_dim=FEATURE_SIZE,\n",
    "            ),\n",
    "            keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define a function that outputs the next token's probability given the\n",
    "    # input sequence.\n",
    "    def token_probability_fn(inputs):\n",
    "        return model(inputs)[:, -1, :]\n",
    "\n",
    "    prompt = tf.fill((BATCH_SIZE, 1), START_ID)\n",
    "\n",
    "    # Print the generated sequence (token ids).\n",
    "    keras_nlp.utils.greedy_search(\n",
    "        token_probability_fn,\n",
    "        prompt,\n",
    "        max_length=10,\n",
    "        end_token_id=END_ID,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "    prompt = _validate_prompt(prompt)\n",
    "\n",
    "    input_is_1d = prompt.shape.rank == 1\n",
    "    if input_is_1d:\n",
    "        prompt = prompt[tf.newaxis, :]\n",
    "\n",
    "    batch_size, length = _get_prompt_shape(prompt)\n",
    "    prompt, mask = _pad_prompt(prompt, max_length)\n",
    "\n",
    "    _validate_token_probability_fn(token_probability_fn, prompt)\n",
    "\n",
    "    def one_step(length, prompt):\n",
    "        pred = token_probability_fn(prompt[:, :length])\n",
    "        next_token = tf.cast(tf.argmax(pred, axis=-1), dtype=prompt.dtype)\n",
    "        next_token = tf.where(mask[:, length], prompt[:, length], next_token)\n",
    "\n",
    "        # Append the next token to current sequence.\n",
    "        prompt = tf.tensor_scatter_nd_update(\n",
    "            tensor=prompt,\n",
    "            indices=tf.stack(\n",
    "                (\n",
    "                    tf.cast(tf.range(batch_size), dtype=length.dtype),\n",
    "                    tf.repeat(length, batch_size),\n",
    "                ),\n",
    "                axis=1,\n",
    "            ),\n",
    "            updates=next_token,\n",
    "        )\n",
    "\n",
    "        length = tf.add(length, 1)\n",
    "        return (length, prompt)\n",
    "\n",
    "    # Run a while loop till text of length `max_length` has been generated.\n",
    "    length, prompt = tf.while_loop(\n",
    "        cond=lambda length, _: tf.less(length, max_length),\n",
    "        body=one_step,\n",
    "        loop_vars=(length, prompt),\n",
    "    )\n",
    "\n",
    "    if end_token_id is not None:\n",
    "        prompt = _mask_tokens_after_end_token(\n",
    "            prompt, max_length, end_token_id, pad_token_id\n",
    "        )\n",
    "\n",
    "    return tf.squeeze(prompt) if input_is_1d else prompt\n",
    "\n",
    "\n",
    "def beam_search(\n",
    "    token_probability_fn,\n",
    "    prompt,\n",
    "    max_length,\n",
    "    num_beams,\n",
    "    from_logits=False,\n",
    "    end_token_id=None,\n",
    "    pad_token_id=0,\n",
    "):\n",
    "    \"\"\"Text generation utility based on beam search algorithm.\n",
    "\n",
    "    At each time-step, beam search keeps the beams (sequences) of the top\n",
    "    `num_beams` highest accumulated probabilities, and uses each one of the\n",
    "    beams to predict candidate next tokens.\n",
    "\n",
    "    Args:\n",
    "        token_probability_fn: a callable, which takes in input_sequence\n",
    "            and outputs the probability distribution of the next token. If\n",
    "            `from_logits` set to True, it should output the logits of the next\n",
    "            token. The input shape would be `[batch_size * num_beams, length]`\n",
    "            and the output should be `[batch_size * num_beams, vocab_size]`.\n",
    "        prompt: a list or a Tensor, can be 1D or 2D, the initial tokens to\n",
    "            append generated tokens. The initial beam for beam search.\n",
    "        max_length: int. The max length of generated text.\n",
    "        num_beams: int. The number of beams that should be kept at each\n",
    "            time-step. `num_beams` should be strictly positive.\n",
    "        from_logits: bool. Indicates whether `token_probability_fn` outputs\n",
    "            logits or probabilities.\n",
    "        end_token_id: int, defaults to None. The token marking the end of the\n",
    "            sequence, once encountered the generation is finished for the exact\n",
    "            sequence. If None, every sequence is generated up to `max_length`.\n",
    "            If set, all tokens after encountering `end_token_id` will be\n",
    "            replaced with `pad_token_id`.\n",
    "        pad_token_id: int, defaults to 0. The pad token after `end_token_id`\n",
    "            is received.\n",
    "\n",
    "    Returns:\n",
    "        A 1D int Tensor, or 2D int Tensor representing the generated\n",
    "        sequences.\n",
    "\n",
    "    Examples:\n",
    "    ```python\n",
    "    BATCH_SIZE = 8\n",
    "    VOCAB_SIZE = 10\n",
    "    FEATURE_SIZE = 16\n",
    "    START_ID = 1\n",
    "    END_ID = 2\n",
    "\n",
    "    # Create a dummy model to predict the next token.\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=[None]),\n",
    "            keras.layers.Embedding(\n",
    "                input_dim=VOCAB_SIZE,\n",
    "                output_dim=FEATURE_SIZE,\n",
    "            ),\n",
    "            keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define a function that outputs the next token's probability given the\n",
    "    # input sequence.\n",
    "    def token_probability_fn(inputs):\n",
    "        return model(inputs)[:, -1, :]\n",
    "\n",
    "    prompt = tf.fill((BATCH_SIZE, 1), START_ID)\n",
    "\n",
    "    # Print the generated sequence (token ids).\n",
    "    keras_nlp.utils.beam_search(\n",
    "        token_probability_fn,\n",
    "        prompt,\n",
    "        max_length=10,\n",
    "        num_beams=5,\n",
    "        end_token_id=END_ID,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "    if num_beams <= 0:\n",
    "        raise ValueError(\n",
    "            f\"`num_beams` should be strictly positive. Received: `num_beams={num_beams}`.\"\n",
    "        )\n",
    "    if num_beams == 1:\n",
    "        return greedy_search(\n",
    "            token_probability_fn=token_probability_fn,\n",
    "            prompt=prompt,\n",
    "            max_length=max_length,\n",
    "            end_token_id=end_token_id,\n",
    "            pad_token_id=pad_token_id,\n",
    "        )\n",
    "\n",
    "    prompt = _validate_prompt(prompt)\n",
    "\n",
    "    input_is_1d = prompt.shape.rank == 1\n",
    "    if input_is_1d:\n",
    "        prompt = prompt[tf.newaxis, :]\n",
    "\n",
    "    batch_size, length = _get_prompt_shape(prompt)\n",
    "    prompt, mask = _pad_prompt(prompt, max_length)\n",
    "\n",
    "    vocab_size, pred_dtype = _validate_token_probability_fn(\n",
    "        token_probability_fn, prompt\n",
    "    )\n",
    "\n",
    "    if length >= max_length:\n",
    "        return tf.squeeze(prompt) if input_is_1d else prompt\n",
    "\n",
    "    # Initialize beam with shape `(batch_size, num_beams, length)`.\n",
    "    beams = tf.repeat(tf.expand_dims(prompt, axis=1), num_beams, axis=1)\n",
    "\n",
    "    # Initialize `beams_prob` with shape `(batch_size, num_beams)`.\n",
    "    beams_prob = tf.zeros([batch_size, 1], dtype=pred_dtype)\n",
    "    beams_prob = tf.concat(\n",
    "        [beams_prob, tf.fill((batch_size, num_beams - 1), pred_dtype.min)],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    def one_step(beams, beams_prob, length):\n",
    "        truncated_beams = beams[..., :length]\n",
    "\n",
    "        flattened_beams = tf.reshape(\n",
    "            truncated_beams, shape=[batch_size * num_beams, -1]\n",
    "        )\n",
    "        preds = token_probability_fn(flattened_beams)\n",
    "        if from_logits:\n",
    "            preds = keras.activations.softmax(preds, axis=-1)\n",
    "        # Reshape `preds` to shape `(batch_size, num_beams * vocab_size)`.\n",
    "        preds = tf.reshape(preds, shape=[batch_size, -1])\n",
    "\n",
    "        probs = tf.math.log(preds) + tf.repeat(\n",
    "            beams_prob, repeats=vocab_size, axis=1\n",
    "        )\n",
    "\n",
    "        candidate_prob, candidate_indexes = tf.math.top_k(\n",
    "            probs, k=num_beams, sorted=False\n",
    "        )\n",
    "        candidate_beam_indexes = candidate_indexes // vocab_size\n",
    "        next_token = candidate_indexes % vocab_size\n",
    "\n",
    "        beams = tf.gather(beams, candidate_beam_indexes, axis=1, batch_dims=1)\n",
    "\n",
    "        # Build a new column of updates to scatter into the beam tensor.\n",
    "        next_token = tf.where(\n",
    "            condition=mask[..., length, tf.newaxis],\n",
    "            x=beams[..., length],\n",
    "            y=next_token,\n",
    "        )\n",
    "        next_token = tf.reshape(next_token, shape=[-1])\n",
    "\n",
    "        # Generate `(batch_index, beam_index)` tuples for each beam.\n",
    "        beam_indices = tf.where(tf.ones((batch_size, num_beams), tf.bool))\n",
    "        beam_indices = tf.cast(beam_indices, dtype=length.dtype)\n",
    "        # Build a tensor of repeated `length` values.\n",
    "        length_indices = tf.fill((batch_size * num_beams, 1), length)\n",
    "        # Concatenate to a triplet of `(batch_index, beam_index, length)`.\n",
    "        indices = tf.concat([beam_indices, length_indices], axis=-1)\n",
    "\n",
    "        # Update `beams[:, :, length]` with `next_token`.\n",
    "        beams = tf.tensor_scatter_nd_update(\n",
    "            tensor=beams,\n",
    "            indices=indices,\n",
    "            updates=next_token,\n",
    "        )\n",
    "\n",
    "        beams_prob = candidate_prob\n",
    "        length = tf.add(length, 1)\n",
    "\n",
    "        return beams, beams_prob, length\n",
    "\n",
    "    # Run a while loop till text of length `max_length` has been generated.\n",
    "    beams, beams_prob, length = tf.while_loop(\n",
    "        cond=lambda beams, beams_prob, length: tf.less(length, max_length),\n",
    "        body=one_step,\n",
    "        loop_vars=(beams, beams_prob, length),\n",
    "    )\n",
    "\n",
    "    # Get the beam with the maximum probability.\n",
    "    max_indexes = tf.math.argmax(beams_prob, axis=-1)\n",
    "    max_beams = tf.gather(\n",
    "        beams, max_indexes[:, tf.newaxis], axis=1, batch_dims=1\n",
    "    )\n",
    "    prompt = tf.squeeze(max_beams)\n",
    "\n",
    "    if end_token_id is not None:\n",
    "        prompt = _mask_tokens_after_end_token(\n",
    "            prompt, max_length, end_token_id, pad_token_id\n",
    "        )\n",
    "    return tf.squeeze(prompt) if input_is_1d else prompt\n",
    "\n",
    "\n",
    "def random_search(\n",
    "    token_probability_fn,\n",
    "    prompt,\n",
    "    max_length,\n",
    "    seed=None,\n",
    "    from_logits=False,\n",
    "    end_token_id=None,\n",
    "    pad_token_id=0,\n",
    "):\n",
    "    \"\"\"Text generation utility based on randomly sampling the entire probability\n",
    "    distribution.\n",
    "\n",
    "    Random sampling samples the next token from the probability distribution\n",
    "    provided by `token_probability_fn` and appends it to the existing sequence.\n",
    "\n",
    "    Args:\n",
    "        token_probability_fn: a callable, which takes in input_sequence\n",
    "            and output the probability distribution of the next token. If\n",
    "            `from_logits` set to True, it should output the logits of the next\n",
    "            token.\n",
    "        prompt: a list or a Tensor, can be 1D or 2D, the initial tokens to\n",
    "            append generated tokens.\n",
    "        max_length: int. The max length of generated text.\n",
    "        seed: int, defaults to None. The random seed used for sampling.\n",
    "        from_logits: bool. Indicates whether `token_probability_fn` outputs\n",
    "            logits or probabilities.\n",
    "        end_token_id: int, defaults to None. The token marking the end of the\n",
    "            sequence, once encountered the generation is finished for the exact\n",
    "            sequence. If None, every sequence is generated up to `max_length`.\n",
    "            If set, all tokens after encountering `end_token_id` will be\n",
    "            replaced with `pad_token_id`.\n",
    "        pad_token_id: int, defaults to 0. The pad token after `end_token_id`\n",
    "            is received.\n",
    "\n",
    "    Returns:\n",
    "        A 1D int Tensor, or 2D int Tensor representing the generated\n",
    "        sequences.\n",
    "\n",
    "    Examples:\n",
    "    ```python\n",
    "    BATCH_SIZE = 8\n",
    "    VOCAB_SIZE = 10\n",
    "    FEATURE_SIZE = 16\n",
    "    START_ID = 1\n",
    "    END_ID = 2\n",
    "\n",
    "    # Create a dummy model to predict the next token.\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=[None]),\n",
    "            keras.layers.Embedding(\n",
    "                input_dim=VOCAB_SIZE,\n",
    "                output_dim=FEATURE_SIZE,\n",
    "            ),\n",
    "            keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define a function that outputs the next token's probability given the\n",
    "    # input sequence.\n",
    "    def token_probability_fn(inputs):\n",
    "        return model(inputs)[:, -1, :]\n",
    "\n",
    "    prompt = tf.fill((BATCH_SIZE, 1), START_ID)\n",
    "\n",
    "    # Print the generated sequence (token ids).\n",
    "    keras_nlp.utils.random_search(\n",
    "        token_probability_fn,\n",
    "        prompt,\n",
    "        max_length=10,\n",
    "        end_token_id=END_ID,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "    prompt = _validate_prompt(prompt)\n",
    "\n",
    "    input_is_1d = prompt.shape.rank == 1\n",
    "    if input_is_1d:\n",
    "        prompt = prompt[tf.newaxis, :]\n",
    "\n",
    "    batch_size, length = _get_prompt_shape(prompt)\n",
    "    prompt, mask = _pad_prompt(prompt, max_length)\n",
    "\n",
    "    _validate_token_probability_fn(token_probability_fn, prompt)\n",
    "\n",
    "    def one_step(length, prompt):\n",
    "        pred = token_probability_fn(prompt[:, :length])\n",
    "        if from_logits:\n",
    "            pred = keras.activations.softmax(pred, axis=-1)\n",
    "        next_token = tf.squeeze(\n",
    "            tf.cast(\n",
    "                tf.random.categorical(tf.math.log(pred), 1, seed=seed),\n",
    "                dtype=prompt.dtype,\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        next_token = tf.where(mask[:, length], prompt[:, length], next_token)\n",
    "\n",
    "        # Append the next token to current sequence.\n",
    "        prompt = tf.tensor_scatter_nd_update(\n",
    "            tensor=prompt,\n",
    "            indices=tf.stack(\n",
    "                (\n",
    "                    tf.cast(tf.range(batch_size), dtype=length.dtype),\n",
    "                    tf.repeat(length, batch_size),\n",
    "                ),\n",
    "                axis=1,\n",
    "            ),\n",
    "            updates=next_token,\n",
    "        )\n",
    "\n",
    "        length = tf.add(length, 1)\n",
    "        return (length, prompt)\n",
    "\n",
    "    # Run a while loop till text of length `max_length` has been generated.\n",
    "    length, prompt = tf.while_loop(\n",
    "        cond=lambda length, _: tf.less(length, max_length),\n",
    "        body=one_step,\n",
    "        loop_vars=(length, prompt),\n",
    "    )\n",
    "\n",
    "    if end_token_id is not None:\n",
    "        prompt = _mask_tokens_after_end_token(\n",
    "            prompt, max_length, end_token_id, pad_token_id\n",
    "        )\n",
    "\n",
    "    return tf.squeeze(prompt) if input_is_1d else prompt\n",
    "\n",
    "\n",
    "def top_k_search(\n",
    "    token_probability_fn,\n",
    "    prompt,\n",
    "    max_length,\n",
    "    k,\n",
    "    seed=None,\n",
    "    from_logits=False,\n",
    "    end_token_id=None,\n",
    "    pad_token_id=0,\n",
    "):\n",
    "    \"\"\"Text generation utility based on top-k sampling.\n",
    "\n",
    "    Top-k search samples the next token from the top-k tokens in the\n",
    "    probability distribution provided by `token_probability_fn` and appends it\n",
    "    to the existing sequence.\n",
    "\n",
    "    Args:\n",
    "        token_probability_fn: a callable, which takes in input_sequence\n",
    "            and output the probability distribution of the next token. If\n",
    "            `from_logits` set to True, it should output the logits of the next\n",
    "            token.\n",
    "        prompt: a list or a Tensor, can be 1D or 2D, the initial tokens to\n",
    "            append generated tokens.\n",
    "        max_length: int. The max length of generated text.\n",
    "        k: int. The number of top tokens to sample from. Should be non-negative\n",
    "            and less than the vocabulary size.\n",
    "        seed: int, defaults to None. The random seed used for sampling.\n",
    "        from_logits: bool. Indicates whether `token_probability_fn` outputs\n",
    "            logits or probabilities.\n",
    "        end_token_id: int, defaults to None. The token marking the end of the\n",
    "            sequence, once encountered the generation is finished for the exact\n",
    "            sequence. If None, every sequence is generated up to `max_length`.\n",
    "            If set, all tokens after encountering `end_token_id` will be\n",
    "            replaced with `pad_token_id`.\n",
    "        pad_token_id: int, defaults to 0. The pad token after `end_token_id`\n",
    "            is received.\n",
    "\n",
    "    Returns:\n",
    "        A 1D int Tensor, or 2D int Tensor representing the generated\n",
    "        sequences.\n",
    "\n",
    "    Examples:\n",
    "    ```python\n",
    "    BATCH_SIZE = 8\n",
    "    VOCAB_SIZE = 10\n",
    "    FEATURE_SIZE = 16\n",
    "    START_ID = 1\n",
    "    END_ID = 2\n",
    "\n",
    "    # Create a dummy model to predict the next token.\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=[None]),\n",
    "            keras.layers.Embedding(\n",
    "                input_dim=VOCAB_SIZE,\n",
    "                output_dim=FEATURE_SIZE,\n",
    "            ),\n",
    "            keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define a function that outputs the next token's probability given the\n",
    "    # input sequence.\n",
    "    def token_probability_fn(inputs):\n",
    "        return model(inputs)[:, -1, :]\n",
    "\n",
    "    prompt = tf.fill((BATCH_SIZE, 1), START_ID)\n",
    "\n",
    "    # Print the generated sequence (token ids).\n",
    "    keras_nlp.utils.top_k_search(\n",
    "        token_probability_fn,\n",
    "        prompt,\n",
    "        max_length=10,\n",
    "        k=4,\n",
    "        end_token_id=END_ID,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "    if k <= 0:\n",
    "        raise ValueError(f\"`k` should be strictly positive. Received: `k={k}`.\")\n",
    "\n",
    "    prompt = _validate_prompt(prompt)\n",
    "\n",
    "    input_is_1d = prompt.shape.rank == 1\n",
    "    if input_is_1d:\n",
    "        prompt = prompt[tf.newaxis, :]\n",
    "\n",
    "    batch_size, length = _get_prompt_shape(prompt)\n",
    "    prompt, mask = _pad_prompt(prompt, max_length)\n",
    "\n",
    "    _validate_token_probability_fn(token_probability_fn, prompt)\n",
    "\n",
    "    # If k is greater than the vocabulary size, use the entire vocabulary.\n",
    "    pred = token_probability_fn(prompt)\n",
    "    if k > pred.shape[1]:\n",
    "        logging.warning(\n",
    "            f\"`k` larger than vocabulary size={pred.shape[1]}.\"\n",
    "            f\"Setting `k` to vocabulary size. Received: `k={k}`.\"\n",
    "        )\n",
    "        k = pred.shape[1]\n",
    "\n",
    "    def one_step(length, prompt):\n",
    "        pred = token_probability_fn(prompt[:, :length])\n",
    "        if from_logits:\n",
    "            pred = keras.activations.softmax(pred, axis=-1)\n",
    "\n",
    "        # Filter out top-k tokens.\n",
    "        top_k_pred, top_k_indices = tf.math.top_k(pred, k=k, sorted=False)\n",
    "        # Sample the next token from the probability distribution.\n",
    "        next_token = tf.random.categorical(\n",
    "            tf.math.log(top_k_pred), 1, seed=seed\n",
    "        )\n",
    "\n",
    "        # Rearrange to get the next token idx from the original order.\n",
    "        next_token = tf.gather_nd(top_k_indices, next_token, batch_dims=1)\n",
    "        next_token = tf.cast(next_token, dtype=prompt.dtype)\n",
    "        next_token = tf.where(mask[:, length], prompt[:, length], next_token)\n",
    "\n",
    "        # Append the next token to current sequence.\n",
    "        prompt = tf.tensor_scatter_nd_update(\n",
    "            tensor=prompt,\n",
    "            indices=tf.stack(\n",
    "                (\n",
    "                    tf.cast(tf.range(batch_size), dtype=length.dtype),\n",
    "                    tf.repeat(length, batch_size),\n",
    "                ),\n",
    "                axis=1,\n",
    "            ),\n",
    "            updates=next_token,\n",
    "        )\n",
    "\n",
    "        length = tf.add(length, 1)\n",
    "        return (length, prompt)\n",
    "\n",
    "    # Run a while loop till text of length `max_length` has been generated.\n",
    "    length, prompt = tf.while_loop(\n",
    "        cond=lambda length, _: tf.less(length, max_length),\n",
    "        body=one_step,\n",
    "        loop_vars=(length, prompt),\n",
    "    )\n",
    "\n",
    "    if end_token_id is not None:\n",
    "        prompt = _mask_tokens_after_end_token(\n",
    "            prompt, max_length, end_token_id, pad_token_id\n",
    "        )\n",
    "\n",
    "    return tf.squeeze(prompt) if input_is_1d else prompt\n",
    "\n",
    "\n",
    "def top_p_search(\n",
    "    token_probability_fn,\n",
    "    prompt,\n",
    "    max_length,\n",
    "    p,\n",
    "    seed=None,\n",
    "    from_logits=False,\n",
    "    end_token_id=None,\n",
    "    pad_token_id=0,\n",
    "):\n",
    "    \"\"\"Text generation utility based on top-p (nucleus) sampling.\n",
    "\n",
    "    Top-p search selects tokens from the smallest subset of output probabilities\n",
    "    that sum to greater than `p`. Put another way, top-p will first order\n",
    "    token predictions by likelihood, and ignore all tokens after the cumulative\n",
    "    probability of selected tokens exceeds `p`. The probability of each\n",
    "    token is provided by `token_probability_fn`.\n",
    "\n",
    "    Args:\n",
    "        token_probability_fn: a callable, which takes in input_sequence\n",
    "            and output the probability distribution of the next token. If\n",
    "            `from_logits` set to True, it should output the logits of the next\n",
    "            token.\n",
    "        prompt: a list or a Tensor, can be 1D or 2D, the initial tokens to\n",
    "            append generated tokens.\n",
    "        max_length: int. The max length of generated text.\n",
    "        p: float. The probability that the top tokens sums up to. Should\n",
    "            follow the constraint of 0 < p < 1.\n",
    "        seed: int, defaults to None. The random seed used for sampling.\n",
    "        from_logits: bool. Indicates whether `token_probability_fn` outputs\n",
    "            logits or probabilities.\n",
    "        end_token_id: int, defaults to None. The token marking the end of the\n",
    "            sequence, once encountered the generation is finished for the exact\n",
    "            sequence. If None, every sequence is generated up to `max_length`.\n",
    "            If set, all tokens after encountering `end_token_id` will be\n",
    "            replaced with `pad_token_id`.\n",
    "        pad_token_id: int, defaults to 0. The pad token after `end_token_id`\n",
    "            is received.\n",
    "\n",
    "    Returns:\n",
    "        A 1D int Tensor, or 2D int Tensor representing the generated\n",
    "        sequences.\n",
    "\n",
    "    Examples:\n",
    "    ```python\n",
    "    BATCH_SIZE = 8\n",
    "    VOCAB_SIZE = 10\n",
    "    FEATURE_SIZE = 16\n",
    "    START_ID = 1\n",
    "    END_ID = 2\n",
    "\n",
    "    # Create a dummy model to predict the next token.\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=[None]),\n",
    "            keras.layers.Embedding(\n",
    "                input_dim=VOCAB_SIZE,\n",
    "                output_dim=FEATURE_SIZE,\n",
    "            ),\n",
    "            keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Define a function that outputs the next token's probability given the\n",
    "    # input sequence.\n",
    "    def token_probability_fn(inputs):\n",
    "        return model(inputs)[:, -1, :]\n",
    "\n",
    "    prompt = tf.fill((BATCH_SIZE, 1), START_ID)\n",
    "\n",
    "    # Print the generated sequence (token ids).\n",
    "    keras_nlp.utils.top_p_search(\n",
    "        token_probability_fn,\n",
    "        prompt,\n",
    "        max_length=10,\n",
    "        p=0.8,\n",
    "        end_token_id=END_ID,\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "    if p <= 0 or p >= 1:\n",
    "        raise ValueError(\n",
    "            f\"`p` should be in the range (0, 1). Received: `p={p}`.\"\n",
    "        )\n",
    "\n",
    "    prompt = _validate_prompt(prompt)\n",
    "\n",
    "    input_is_1d = prompt.shape.rank == 1\n",
    "    if input_is_1d:\n",
    "        prompt = prompt[tf.newaxis, :]\n",
    "\n",
    "    batch_size, length = _get_prompt_shape(prompt)\n",
    "    prompt, mask = _pad_prompt(prompt, max_length)\n",
    "\n",
    "    _validate_token_probability_fn(token_probability_fn, prompt)\n",
    "\n",
    "    def one_step(length, prompt):\n",
    "        pred = token_probability_fn(prompt[:, :length])\n",
    "        if from_logits:\n",
    "            pred = keras.activations.softmax(pred, axis=-1)\n",
    "        # Sort preds in descending order.\n",
    "        sorted_preds, sorted_indices = tf.math.top_k(\n",
    "            pred, k=pred.shape[1], sorted=True\n",
    "        )\n",
    "        # Calculate cumulative probability distribution.\n",
    "        cumulative_probs = tf.math.cumsum(sorted_preds, axis=-1)\n",
    "        # Create a mask for the tokens to keep.\n",
    "        keep_mask = cumulative_probs <= p\n",
    "        # Shift to include the last token that exceed p.\n",
    "        shifted_keep_mask = tf.concat(\n",
    "            [tf.ones_like(keep_mask[:, :1]), keep_mask[:, :-1]], axis=-1\n",
    "        )\n",
    "        # Filter out unmasked tokens and sample from filtered distribution.\n",
    "        probs = tf.where(\n",
    "            shifted_keep_mask,\n",
    "            sorted_preds,\n",
    "            tf.zeros(tf.shape(pred), dtype=sorted_preds.dtype),\n",
    "        )\n",
    "        sorted_next_token = tf.random.categorical(\n",
    "            tf.math.log(probs), 1, seed=seed\n",
    "        )\n",
    "        next_token = tf.gather_nd(\n",
    "            sorted_indices, sorted_next_token, batch_dims=1\n",
    "        )\n",
    "        next_token = tf.cast(next_token, dtype=prompt.dtype)\n",
    "        next_token = tf.where(mask[:, length], prompt[:, length], next_token)\n",
    "\n",
    "        # Append the next token to current sequence.\n",
    "        prompt = tf.tensor_scatter_nd_update(\n",
    "            tensor=prompt,\n",
    "            indices=tf.stack(\n",
    "                (\n",
    "                    tf.cast(tf.range(batch_size), dtype=length.dtype),\n",
    "                    tf.repeat(length, batch_size),\n",
    "                ),\n",
    "                axis=1,\n",
    "            ),\n",
    "            updates=next_token,\n",
    "        )\n",
    "\n",
    "        length = tf.add(length, 1)\n",
    "        return (length, prompt)\n",
    "\n",
    "    # Run a while loop till text of length `max_length` has been generated.\n",
    "    length, prompt = tf.while_loop(\n",
    "        cond=lambda length, _: tf.less(length, max_length),\n",
    "        body=one_step,\n",
    "        loop_vars=(length, prompt),\n",
    "    )\n",
    "\n",
    "    if end_token_id is not None:\n",
    "        prompt = _mask_tokens_after_end_token(\n",
    "            prompt, max_length, end_token_id, pad_token_id\n",
    "        )\n",
    "\n",
    "    return tf.squeeze(prompt) if input_is_1d else prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Example 0 **\n",
      "tom doesn't have a cat.\n",
      "tom no tiene una .\n",
      "\n",
      "** Example 1 **\n",
      "she was trembling with fear.\n",
      "ella no fue aricumpro .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def decode_sequences(input_sentences):\n",
    "    batch_size = tf.shape(input_sentences)[0]\n",
    "\n",
    "    # Tokenize the encoder input.\n",
    "    encoder_input_tokens = eng_tokenizer(input_sentences).to_tensor(\n",
    "        shape=(None, MAX_SEQUENCE_LENGTH)\n",
    "    )\n",
    "\n",
    "    # Define a function that outputs the next token's probability given the\n",
    "    # input sequence.\n",
    "    def token_probability_fn(decoder_input_tokens):\n",
    "        return transformer([encoder_input_tokens, decoder_input_tokens])[:, -1, :]\n",
    "\n",
    "    # Set the prompt to the \"[START]\" token.\n",
    "    prompt = tf.fill((batch_size, 1), spa_tokenizer.token_to_id(\"[START]\"))\n",
    "\n",
    "    generated_tokens = top_p_search(\n",
    "        token_probability_fn,\n",
    "        prompt,\n",
    "        p=0.1,\n",
    "        max_length=40,\n",
    "        end_token_id=spa_tokenizer.token_to_id(\"[END]\"),\n",
    "    )\n",
    "    generated_sentences = spa_tokenizer.detokenize(generated_tokens)\n",
    "    return generated_sentences\n",
    "\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for i in range(10):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    translated = decode_sequences(tf.constant([input_sentence]))\n",
    "    translated = translated.numpy()[0].decode(\"utf-8\")\n",
    "    translated = (\n",
    "        translated.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    print(f\"** Example {i} **\")\n",
    "    print(input_sentence)\n",
    "    print(translated)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Score:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.37296093>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.3413588>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.35042742>}\n",
      "ROUGE-2 Score:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.18063973>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.16484126>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.16860461>}\n"
     ]
    }
   ],
   "source": [
    "rouge_1 = keras_nlp.metrics.RougeN(order=1)\n",
    "rouge_2 = keras_nlp.metrics.RougeN(order=2)\n",
    "\n",
    "for test_pair in test_pairs[:30]:\n",
    "    input_sentence = test_pair[0]\n",
    "    reference_sentence = test_pair[1]\n",
    "\n",
    "    translated_sentence = decode_sequences(tf.constant([input_sentence]))\n",
    "    translated_sentence = translated_sentence.numpy()[0].decode(\"utf-8\")\n",
    "    translated_sentence = (\n",
    "        translated_sentence.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    rouge_1(reference_sentence, translated_sentence)\n",
    "    rouge_2(reference_sentence, translated_sentence)\n",
    "\n",
    "print(\"ROUGE-1 Score: \", rouge_1.result())\n",
    "print(\"ROUGE-2 Score: \", rouge_2.result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
